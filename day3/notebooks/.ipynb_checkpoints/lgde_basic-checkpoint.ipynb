{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스파크를 이용한 기본 지표 생성 예제\n",
    "> 기본 지표를 생성하는 데에 있어, 정해진 틀을 그대로 따라하기 보다, 가장 직관적인 방법을 지속적으로 개선하는 과정을 설명하기 위한 예제입니다. \n",
    "첫 번째 예제인 만큼 지표의 복잡도를 줄이기 위해 해당 서비스를 오픈 일자는 2020/10/25 이며, 지표를 집계하는 시점은 2020/10/26 일 입니다\n",
    "\n",
    "* 원본 데이터를 그대로 읽는 방법\n",
    "* dataframe api 를 이용하는 방법\n",
    "* spark.sql 을 이용하는 방법\n",
    "* 기본 지표 (DAU, PU)를 추출하는 예제 실습\n",
    "* 날짜에 대한 필터를 넣는 방법\n",
    "* 날짜에 대한 필터를 데이터 소스에 적용하는 방법\n",
    "* 기본 지표 (ARPU, ARPPU)를 추출하는 예제 실습\n",
    "* 스칼라 값을 가져와서 다음 질의문에 적용하는 방법\n",
    "* 누적 금액을 구할 때에 단순한 방법\n",
    "* 서비스 오픈 일자의 디멘젼 테이블을 생성하는 방법\n",
    "* 널 값에 대한 처리하는 방법\n",
    "* 생성된 데이터 프레임을 저장하는 방법\n",
    "* 전 일자 데이터를 가져오는 방법\n",
    "* 요약 지표를 생성할 때에 단순한 방법\n",
    "* 팩트 테이블을 활용하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data Engineer Basic Day3\") \\\n",
    "    .config(\"spark.dataengineer.basic.day3\", \"tutorial-1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+------+\n",
      "|             a_time|a_uid|  a_id|\n",
      "+-------------------+-----+------+\n",
      "|2020-10-25 17:00:00|    1| login|\n",
      "|2020-10-25 17:33:20|    1|logout|\n",
      "|2020-10-25 18:06:40|    2| login|\n",
      "|2020-10-25 18:23:20|    2|logout|\n",
      "|2020-10-25 19:13:20|    2| login|\n",
      "|2020-10-25 20:20:00|    3| login|\n",
      "|2020-10-25 20:53:20|    3|logout|\n",
      "|2020-10-25 21:10:00|    4| login|\n",
      "|2020-10-25 22:16:40|    4|logout|\n",
      "|2020-10-25 22:21:40|    4| login|\n",
      "|2020-10-25 22:55:00|    5| login|\n",
      "|2020-10-25 23:45:00|    5|logout|\n",
      "|2020-10-26 00:01:40|    6| login|\n",
      "|2020-10-26 00:51:40|    7| login|\n",
      "|2020-10-26 01:08:20|    8| login|\n",
      "|2020-10-26 01:25:00|    9| login|\n",
      "+-------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"data/log_access.csv\").withColumn(\"a_time\", expr(\"from_unixtime(a_time)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"data/tbl_user.csv\").createOrReplaceTempView(\"user\")\n",
    "\n",
    "pWhere=\"\"\n",
    "spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"data/tbl_purchase.csv\").withColumn(\"p_time\", expr(\"from_unixtime(p_time)\")).createOrReplaceTempView(\"purchase\")\n",
    "\n",
    "aWhere=\"\"\n",
    "spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"data/log_access.csv\").withColumn(\"a_time\", expr(\"from_unixtime(a_time)\")).createOrReplaceTempView(\"access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|    u_id|      int|   null|\n",
      "|  u_name|   string|   null|\n",
      "|u_gender|   string|   null|\n",
      "|u_signup|      int|   null|\n",
      "+--------+---------+-------+\n",
      "\n",
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|  p_time|   string|   null|\n",
      "|   p_uid|      int|   null|\n",
      "|    p_id|      int|   null|\n",
      "|  p_name|   string|   null|\n",
      "|p_amount|      int|   null|\n",
      "+--------+---------+-------+\n",
      "\n",
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|  a_time|   string|   null|\n",
      "|   a_uid|      int|   null|\n",
      "|    a_id|   string|   null|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"desc user\").show()\n",
    "spark.sql(\"desc purchase\").show()\n",
    "spark.sql(\"desc access\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과제 1. 주어진 데이터를 이용하여 2020/10/25 기준의 DAU, PU 를 구하시오\n",
    "* DAU : Daily Active User, 일 별 접속자 수\n",
    "  - log_access 를 통해 unique 한 a_uid 값을 구합니다\n",
    "* PU : Purchase User, 일 별 구매자 수\n",
    "  - tbl_purchase 를 통해 unique 한 p_uid 값을 구합니다\n",
    "\n",
    "> 값을 구하기 전에 Spark API 대신 Spark SQL 을 이용하기 위해 [createOrReplaceTempView](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=createorreplace#pyspark.sql.DataFrame.createOrReplaceTempView) 를 생성합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|             a_time|a_uid|\n",
      "+-------------------+-----+\n",
      "|2020-10-25 17:00:00|    1|\n",
      "|2020-10-25 17:33:20|    1|\n",
      "|2020-10-25 18:06:40|    2|\n",
      "|2020-10-25 18:23:20|    2|\n",
      "|2020-10-25 19:13:20|    2|\n",
      "|2020-10-25 20:20:00|    3|\n",
      "|2020-10-25 20:53:20|    3|\n",
      "|2020-10-25 21:10:00|    4|\n",
      "|2020-10-25 22:16:40|    4|\n",
      "|2020-10-25 22:21:40|    4|\n",
      "|2020-10-25 22:55:00|    5|\n",
      "|2020-10-25 23:45:00|    5|\n",
      "|2020-10-26 00:01:40|    6|\n",
      "|2020-10-26 00:51:40|    7|\n",
      "|2020-10-26 01:08:20|    8|\n",
      "|2020-10-26 01:25:00|    9|\n",
      "+-------------------+-----+\n",
      "\n",
      "+---+\n",
      "|DAU|\n",
      "+---+\n",
      "|  5|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DAU - access\n",
    "spark.sql(\"select a_time as a_time, a_uid from access\").show()\n",
    "dau = spark.sql(\"select count(distinct a_uid) as DAU from access where a_time >= '2020-10-25 00:00:00' and a_time < '2020-10-26 00:00:00'\")\n",
    "dau.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|             p_time|p_uid|\n",
      "+-------------------+-----+\n",
      "|2020-10-25 18:45:50|    1|\n",
      "|2020-10-26 06:45:55|    1|\n",
      "|2020-10-26 00:51:40|    2|\n",
      "|2020-10-25 18:55:55|    3|\n",
      "|2020-10-26 01:08:20|    4|\n",
      "|2020-10-25 22:45:55|    5|\n",
      "|2020-10-25 22:49:15|    5|\n",
      "+-------------------+-----+\n",
      "\n",
      "+---+\n",
      "| PU|\n",
      "+---+\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PU - purchase\n",
    "spark.sql(\"select p_time, p_uid from purchase\").show()\n",
    "pu = spark.sql(\"select count(distinct p_uid) as PU from purchase where p_time >= '2020-10-25 00:00:00' and p_time < '2020-10-26 00:00:00'\")\n",
    "pu.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_dau = dau.collect()[0][\"DAU\"]\n",
    "v_pu = pu.collect()[0][\"PU\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과제 2. 주어진 데이터를 이용하여 2020/10/25 기준의 ARPU, ARPPU 를 구하시오\n",
    "* ARPU : Average Revenue Per User, 유저 당 평균 수익\n",
    "  - 해당 일자의 전체 수익 (Total Purchase Amount) / 해당 일에 접속한 유저 수 (DAU)\n",
    "* ARPPU : Average Revenue Per Purchase User, 구매 유저 당 평균 수익\n",
    "  - 해당 일자의 전체 수익 (Total Purchase Amount) / 해당 일에 접속한 구매 유저 수 (PU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select sum(p_amount) / 5 from purchase where p_time >= '2020-10-25 00:00:00' and p_time < '2020-10-26 00:00:00'\n",
      "+---------------------+\n",
      "|total_purchase_amount|\n",
      "+---------------------+\n",
      "|              9000000|\n",
      "+---------------------+\n",
      "\n",
      "+-------------------------------------------------------------------+\n",
      "|(CAST(sum(CAST(p_amount AS BIGINT)) AS DOUBLE) / CAST(5 AS DOUBLE))|\n",
      "+-------------------------------------------------------------------+\n",
      "|                                                          1800000.0|\n",
      "+-------------------------------------------------------------------+\n",
      "\n",
      "+---------+\n",
      "|     ARPU|\n",
      "+---------+\n",
      "|1800000.0|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ARPU - total purchase amount, dau\n",
    "\n",
    "query=\"select sum(p_amount) / {} from purchase where p_time >= '2020-10-25 00:00:00' and p_time < '2020-10-26 00:00:00'\".format(v_dau)\n",
    "print(query)\n",
    "\n",
    "total_purchase_amount = spark.sql(\"select sum(p_amount) as total_purchase_amount from purchase where p_time >= '2020-10-25 00:00:00' and p_time < '2020-10-26 00:00:00'\")\n",
    "total_purchase_amount.show()\n",
    "\n",
    "spark.sql(\"select sum(p_amount) / 5 from purchase where p_time >= '2020-10-25 00:00:00' and p_time < '2020-10-26 00:00:00'\").show()\n",
    "\n",
    "spark.sql(\"select sum(p_amount) / {} as ARPU from purchase where p_time >= '2020-10-25 00:00:00' and p_time < '2020-10-26 00:00:00'\".format(v_dau)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ARPPU | 3000000.0 |\n"
     ]
    }
   ],
   "source": [
    "# ARPPU - total purchase amount, pu\n",
    "v_amt = total_purchase_amount.collect()[0][\"total_purchase_amount\"]\n",
    "print(\"| ARPPU | {} |\".format(v_amt / v_pu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과제 3. 주어진 데이터를 이용하여 2020/10/26 현재의 \"누적 매출 금액\" 과 \"누적 접속 유저수\"를 구하시오\n",
    "* 누적 매출 금액 : 10/25 (오픈) ~ 현재\n",
    "  - 전체 로그를 읽어서 매출 금액의 합을 구한다\n",
    "  - 유저별 매출 정보를 누적하여 저장해두고 재활용한다\n",
    "* 누적 접속 유저수 : 10/25 (오픈) ~ 현재\n",
    "  - 전체 로그를 읽어서 접속자의 유일한 수를 구한다\n",
    "  - 유저별 접속 정보를 누적하여 저장해두고 재활용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(p_amount)|\n",
      "+-------------+\n",
      "|     16700000|\n",
      "+-------------+\n",
      "\n",
      "+---------------------+\n",
      "|count(DISTINCT a_uid)|\n",
      "+---------------------+\n",
      "|                    9|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 누적 매출 금액\n",
    "spark.sql(\"select sum(p_amount) from purchase \").show()\n",
    "\n",
    "# 누적 접속 유저수\n",
    "spark.sql(\"select count(distinct a_uid) from access\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Dimension 테이블 설계\n",
    "| 컬럼 명 | 컬럼 타입 | 컬럼 설명 |\n",
    "| :- | :-: | :- |\n",
    "| d_uid | int | 유저 아이디 |\n",
    "| d_name | string | 고객 이름 |\n",
    "| d_pamount | int | 누적 구매 금액 |\n",
    "| d_pcount | int | 누적 구매 횟수 |\n",
    "| d_acount | int | 누적 접속 횟수 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|  a_time|   string|   null|\n",
      "|   a_uid|      int|   null|\n",
      "|    a_id|   string|   null|\n",
      "+--------+---------+-------+\n",
      "\n",
      "+-------------------+-----+-----+\n",
      "|             a_time|a_uid| a_id|\n",
      "+-------------------+-----+-----+\n",
      "|2020-10-25 17:00:00|    1|login|\n",
      "|2020-10-25 18:06:40|    2|login|\n",
      "|2020-10-25 19:13:20|    2|login|\n",
      "|2020-10-25 20:20:00|    3|login|\n",
      "|2020-10-25 21:10:00|    4|login|\n",
      "|2020-10-25 22:21:40|    4|login|\n",
      "|2020-10-25 22:55:00|    5|login|\n",
      "+-------------------+-----+-----+\n",
      "\n",
      "+-----+------+\n",
      "|a_uid|acount|\n",
      "+-----+------+\n",
      "|    1|     1|\n",
      "|    3|     1|\n",
      "|    5|     1|\n",
      "|    4|     2|\n",
      "|    2|     2|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 오픈 첫 날의 경우 예외적으로 별도의 프로그램을 작성합니다\n",
    "# \n",
    "# 1. 가장 큰 레코드 수를 가진 정보가 접속정보이므로 해당 일자의 이용자 별 접속 횟수를 추출합니다\n",
    "# 단, login 횟수를 접속 횟수로 가정합니다 - logout 만 있는 경우는 login 유실 혹은 전일자의 로그이므로 이러한 경우는 제외합니다\n",
    "spark.sql(\"describe access\").show()\n",
    "spark.sql(\"select * from access where a_id = 'login' and a_time >= '2020-10-25 00:00:00' and a_time < '2020-10-26 00:00:00'\").show()\n",
    "uids = spark.sql(\"select a_uid, count(a_uid) as acount from access where a_id = 'login' and a_time >= '2020-10-25 00:00:00' and a_time < '2020-10-26 00:00:00' group by a_uid\")\n",
    "uids.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|  p_time|   string|   null|\n",
      "|   p_uid|      int|   null|\n",
      "|    p_id|      int|   null|\n",
      "|  p_name|   string|   null|\n",
      "|p_amount|      int|   null|\n",
      "+--------+---------+-------+\n",
      "\n",
      "+-----+-------+------+\n",
      "|p_uid|pamount|pcount|\n",
      "+-----+-------+------+\n",
      "|    1|2000000|     1|\n",
      "|    3|1000000|     1|\n",
      "|    5|6000000|     2|\n",
      "+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. 해당 일자의 이용자 별 총 매출 금액과, 구매 횟수를 추출합니다\n",
    "spark.sql(\"describe purchase\").show()\n",
    "amts = spark.sql(\"select p_uid, sum(p_amount) as pamount, count(p_uid) as pcount from purchase where p_time >= '2020-10-25 00:00:00' and p_time < '2020-10-26 00:00:00' group by p_uid\")\n",
    "amts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a_uid: integer (nullable = true)\n",
      " |-- acount: long (nullable = false)\n",
      "\n",
      "root\n",
      " |-- p_uid: integer (nullable = true)\n",
      " |-- pamount: long (nullable = true)\n",
      " |-- pcount: long (nullable = false)\n",
      "\n",
      "+-----+--------+---------+--------+\n",
      "|d_uid|d_acount|d_pamount|d_pcount|\n",
      "+-----+--------+---------+--------+\n",
      "|    1|       1|  2000000|       1|\n",
      "|    2|       2|     null|    null|\n",
      "|    3|       1|  1000000|       1|\n",
      "|    4|       2|     null|    null|\n",
      "|    5|       1|  6000000|       2|\n",
      "+-----+--------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. 이용자 접속횟수 + 총구매금액 + 구매횟수 (uids + amts)\n",
    "\n",
    "uids.printSchema()\n",
    "amts.printSchema()\n",
    "\n",
    "dim1 = uids.join(amts, uids[\"a_uid\"] == amts[\"p_uid\"], how=\"left\").sort(uids[\"a_uid\"].asc())\n",
    "dim2 = dim1.withColumnRenamed(\"a_uid\", \"d_uid\") \\\n",
    ".withColumnRenamed(\"acount\", \"d_acount\") \\\n",
    ".drop(\"p_uid\") \\\n",
    ".withColumnRenamed(\"pamount\", \"d_pamount\") \\\n",
    ".withColumnRenamed(\"pcount\", \"d_pcount\")\n",
    "dim2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+--------+--------+\n",
      "|u_id|    u_name|u_gender|u_signup|\n",
      "+----+----------+--------+--------+\n",
      "|   1|    정휘센|      남|19580808|\n",
      "|   2|  김싸이언|      남|19590201|\n",
      "|   3|    박트롬|      여|19951030|\n",
      "|   4|    청소기|      남|19770329|\n",
      "|   5|유코드제로|      여|20021029|\n",
      "|   6|  윤디오스|      남|20040101|\n",
      "|   7|  임모바일|      남|20040807|\n",
      "|   8|  조노트북|      여|20161201|\n",
      "|   9|  최컴퓨터|      남|20201124|\n",
      "+----+----------+--------+--------+\n",
      "\n",
      "+-----+----------+--------+--------+---------+--------+\n",
      "|d_uid|    d_name|d_gender|d_acount|d_pamount|d_pcount|\n",
      "+-----+----------+--------+--------+---------+--------+\n",
      "|    1|    정휘센|      남|       1|  2000000|       1|\n",
      "|    2|  김싸이언|      남|       2|        0|       0|\n",
      "|    3|    박트롬|      여|       1|  1000000|       1|\n",
      "|    4|    청소기|      남|       2|        0|       0|\n",
      "|    5|유코드제로|      여|       1|  6000000|       2|\n",
      "+-----+----------+--------+--------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. 이용자 정보를 덧 붙입니다\n",
    "user = spark.sql(\"select * from user\")\n",
    "user.show()\n",
    "\n",
    "dim3 = dim2.join(user, dim2[\"d_uid\"] == user[\"u_id\"], \"left\")\n",
    "dim4 = dim3.withColumnRenamed(\"u_name\", \"d_name\") \\\n",
    ".withColumnRenamed(\"u_gender\", \"d_gender\")\n",
    "\n",
    "dim5 = dim4.select(\"d_uid\", \"d_name\", \"d_gender\", \"d_acount\", \"d_pamount\", \"d_pcount\")\n",
    "dimension = dim5.na.fill({\"d_pamount\":0, \"d_pcount\":0})\n",
    "dimension.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 다음날 해당 데이터를 사용하도록 하기 위해 일자별 경로에 저장합니다\n",
    "# - ./users/dt=20201025/\n",
    "target=\"./users/dt=20201025\"\n",
    "dimension.write.mode(\"overwrite\").parquet(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+--------+---------+--------+\n",
      "|d_uid|    d_name|d_gender|d_acount|d_pamount|d_pcount|\n",
      "+-----+----------+--------+--------+---------+--------+\n",
      "|    1|    정휘센|      남|       1|  2000000|       1|\n",
      "|    2|  김싸이언|      남|       2|        0|       0|\n",
      "|    3|    박트롬|      여|       1|  1000000|       1|\n",
      "|    4|    청소기|      남|       2|        0|       0|\n",
      "|    5|유코드제로|      여|       1|  6000000|       2|\n",
      "+-----+----------+--------+--------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yesterday = spark.read.parquet(target)\n",
    "yesterday.sort(yesterday[\"d_uid\"].asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 다음 날 동일한 지표를 생성하되 이전 일자의 정보에 누적한 지표를 생성합니다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과제 4. 주어진 데이터를 이용하여 2020/10/25 기준의 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과제 5. 주어진 데이터를 이용하여 2020/10/25 기준의 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과제 6. 주어진 데이터를 이용하여 2020/10/25 기준의 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
