{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스파크를 이용한 기본 지표 생성 예제\n",
    "> 기본 지표를 생성하는 데에 있어, 정해진 틀을 그대로 따라하기 보다, 가장 직관적인 방법을 지속적으로 개선하는 과정을 설명하기 위한 예제입니다. \n",
    "첫 번째 예제인 만큼 지표의 복잡도를 줄이기 위해 해당 서비스를 오픈 일자는 2020/10/25 이며, 지표를 집계하는 시점은 2020/10/26 일 입니다\n",
    "\n",
    "* 원본 데이터를 그대로 읽는 방법\n",
    "* dataframe api 를 이용하는 방법\n",
    "* spark.sql 을 이용하는 방법\n",
    "* 기본 지표 (DAU, PU)를 추출하는 예제 실습\n",
    "* 날짜에 대한 필터를 넣는 방법\n",
    "* 날짜에 대한 필터를 데이터 소스에 적용하는 방법\n",
    "* 기본 지표 (ARPU, ARPPU)를 추출하는 예제 실습\n",
    "* 스칼라 값을 가져와서 다음 질의문에 적용하는 방법\n",
    "* 누적 금액을 구할 때에 단순한 방법\n",
    "* 서비스 오픈 일자의 디멘젼 테이블을 생성하는 방법\n",
    "* 널 값에 대한 처리하는 방법\n",
    "* 생성된 데이터 프레임을 저장하는 방법\n",
    "* 전 일자 데이터를 가져오는 방법\n",
    "* 요약 지표를 생성할 때에 단순한 방법\n",
    "* 팩트 테이블을 활용하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data Engineer Basic Day3\") \\\n",
    "    .config(\"spark.dataengineer.basic.day3\", \"tutorial-1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+--------------------+-----+-------------------+-------------------+\n",
      "|  a_id| a_tag|    a_time|         a_timestamp|a_uid|           gmt_time|          localtime|\n",
      "+------+------+----------+--------------------+-----+-------------------+-------------------+\n",
      "|logout|access|1603647200|2020-10-26 02:33:...|    1|2020-10-25 17:33:20|2020-10-25 17:33:20|\n",
      "|logout|access|1603650200|2020-10-26 03:23:...|    2|2020-10-25 18:23:20|2020-10-25 18:23:20|\n",
      "|logout|access|1603659200|2020-10-26 05:53:...|    3|2020-10-25 20:53:20|2020-10-25 20:53:20|\n",
      "|logout|access|1603664200|2020-10-26 07:16:...|    4|2020-10-25 22:16:40|2020-10-25 22:16:40|\n",
      "|logout|access|1603669500|2020-10-26 08:45:...|    5|2020-10-25 23:45:00|2020-10-25 23:45:00|\n",
      "| login|access|1603645200|2020-10-26 02:00:...|    1|2020-10-25 17:00:00|2020-10-25 17:00:00|\n",
      "| login|access|1603649200|2020-10-26 03:06:...|    2|2020-10-25 18:06:40|2020-10-25 18:06:40|\n",
      "| login|access|1603653200|2020-10-26 04:13:...|    2|2020-10-25 19:13:20|2020-10-25 19:13:20|\n",
      "| login|access|1603657200|2020-10-26 05:20:...|    3|2020-10-25 20:20:00|2020-10-25 20:20:00|\n",
      "| login|access|1603660200|2020-10-26 06:10:...|    4|2020-10-25 21:10:00|2020-10-25 21:10:00|\n",
      "| login|access|1603664500|2020-10-26 07:21:...|    4|2020-10-25 22:21:40|2020-10-25 22:21:40|\n",
      "| login|access|1603666500|2020-10-26 07:55:...|    5|2020-10-25 22:55:00|2020-10-25 22:55:00|\n",
      "| login|access|1603670500|2020-10-26 09:01:...|    6|2020-10-26 00:01:40|2020-10-26 00:01:40|\n",
      "| login|access|1603673500|2020-10-26 09:51:...|    7|2020-10-26 00:51:40|2020-10-26 00:51:40|\n",
      "| login|access|1603674500|2020-10-26 10:08:...|    8|2020-10-26 01:08:20|2020-10-26 01:08:20|\n",
      "| login|access|1603675500|2020-10-26 10:25:...|    9|2020-10-26 01:25:00|2020-10-26 01:25:00|\n",
      "+------+------+----------+--------------------+-----+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").json(\"access/20201026\") \\\n",
    ".withColumn(\"gmt_time\", expr(\"from_unixtime(a_time, 'yyyy-MM-dd HH:mm:ss')\")) \\\n",
    ".withColumn(\"localtime\", expr(\"from_utc_timestamp(from_unixtime(a_time), 'Asis/Seoul')\")) \\\n",
    ".show()\n",
    "\n",
    "# from_utc_timestamp(from_unixtime(epoch_time), tz_name) as local_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Asia/Seoul'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.conf.unset(\"spark.sql.session.timeZone\")\n",
    "spark.conf.get(\"spark.sql.session.timeZone\") # 'Etc/UTC' => 이게 원인이었네 ... 초기 값의 TimeZone 설정이 제대로 안 되어 있었음.;ㅁ;\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "spark.conf.get(\"spark.sql.session.timeZone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+--------------------+-----+-------------------+-------------------+\n",
      "|  a_id| a_tag|    a_time|         a_timestamp|a_uid|           gmt_time|          localtime|\n",
      "+------+------+----------+--------------------+-----+-------------------+-------------------+\n",
      "|logout|access|1603647200|2020-10-26 02:33:...|    1|2020-10-26 02:33:20|2020-10-26 02:33:20|\n",
      "|logout|access|1603650200|2020-10-26 03:23:...|    2|2020-10-26 03:23:20|2020-10-26 03:23:20|\n",
      "|logout|access|1603659200|2020-10-26 05:53:...|    3|2020-10-26 05:53:20|2020-10-26 05:53:20|\n",
      "|logout|access|1603664200|2020-10-26 07:16:...|    4|2020-10-26 07:16:40|2020-10-26 07:16:40|\n",
      "|logout|access|1603669500|2020-10-26 08:45:...|    5|2020-10-26 08:45:00|2020-10-26 08:45:00|\n",
      "| login|access|1603645200|2020-10-26 02:00:...|    1|2020-10-26 02:00:00|2020-10-26 02:00:00|\n",
      "| login|access|1603649200|2020-10-26 03:06:...|    2|2020-10-26 03:06:40|2020-10-26 03:06:40|\n",
      "| login|access|1603653200|2020-10-26 04:13:...|    2|2020-10-26 04:13:20|2020-10-26 04:13:20|\n",
      "| login|access|1603657200|2020-10-26 05:20:...|    3|2020-10-26 05:20:00|2020-10-26 05:20:00|\n",
      "| login|access|1603660200|2020-10-26 06:10:...|    4|2020-10-26 06:10:00|2020-10-26 06:10:00|\n",
      "| login|access|1603664500|2020-10-26 07:21:...|    4|2020-10-26 07:21:40|2020-10-26 07:21:40|\n",
      "| login|access|1603666500|2020-10-26 07:55:...|    5|2020-10-26 07:55:00|2020-10-26 07:55:00|\n",
      "| login|access|1603670500|2020-10-26 09:01:...|    6|2020-10-26 09:01:40|2020-10-26 09:01:40|\n",
      "| login|access|1603673500|2020-10-26 09:51:...|    7|2020-10-26 09:51:40|2020-10-26 09:51:40|\n",
      "| login|access|1603674500|2020-10-26 10:08:...|    8|2020-10-26 10:08:20|2020-10-26 10:08:20|\n",
      "| login|access|1603675500|2020-10-26 10:25:...|    9|2020-10-26 10:25:00|2020-10-26 10:25:00|\n",
      "+------+------+----------+--------------------+-----+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").json(\"access/20201026\") \\\n",
    ".withColumn(\"gmt_time\", expr(\"from_unixtime(a_time, 'yyyy-MM-dd HH:mm:ss')\")) \\\n",
    ".withColumn(\"localtime\", expr(\"from_utc_timestamp(from_unixtime(a_time), 'Asis/Seoul')\")) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").parquet(\"user/20201025\").createOrReplaceTempView(\"user\")\n",
    "\n",
    "pWhere=\"\"\n",
    "spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").parquet(\"purchase/20201025\").withColumn(\"p_time\", expr(\"from_unixtime(p_time)\")).createOrReplaceTempView(\"purchase\")\n",
    "\n",
    "aWhere=\"\"\n",
    "spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").json(\"access/20201026\").withColumn(\"a_time\", expr(\"from_unixtime(a_time)\")).createOrReplaceTempView(\"access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|    u_id|      int|   null|\n",
      "|  u_name|   string|   null|\n",
      "|u_gender|   string|   null|\n",
      "|u_signup|      int|   null|\n",
      "+--------+---------+-------+\n",
      "\n",
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|  p_time|   string|   null|\n",
      "|   p_uid|      int|   null|\n",
      "|    p_id|      int|   null|\n",
      "|  p_name|   string|   null|\n",
      "| p_amoun|      int|   null|\n",
      "+--------+---------+-------+\n",
      "\n",
      "+-----------+---------+-------+\n",
      "|   col_name|data_type|comment|\n",
      "+-----------+---------+-------+\n",
      "|       a_id|   string|   null|\n",
      "|      a_tag|   string|   null|\n",
      "|     a_time|   string|   null|\n",
      "|a_timestamp|   string|   null|\n",
      "|      a_uid|   string|   null|\n",
      "+-----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"desc user\").show()\n",
    "spark.sql(\"desc purchase\").show()\n",
    "spark.sql(\"desc access\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과제 1. 주어진 데이터를 이용하여 2020/10/25 기준의 DAU, PU 를 구하시오\n",
    "* DAU : Daily Active User, 일 별 접속자 수\n",
    "  - log_access 를 통해 unique 한 a_uid 값을 구합니다\n",
    "* PU : Purchase User, 일 별 구매자 수\n",
    "  - tbl_purchase 를 통해 unique 한 p_uid 값을 구합니다\n",
    "\n",
    "> 값을 구하기 전에 Spark API 대신 Spark SQL 을 이용하기 위해 [createOrReplaceTempView](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=createorreplace#pyspark.sql.DataFrame.createOrReplaceTempView) 를 생성합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|             a_time|a_uid|\n",
      "+-------------------+-----+\n",
      "|2020-10-25 17:33:20|    1|\n",
      "|2020-10-25 18:23:20|    2|\n",
      "|2020-10-25 20:53:20|    3|\n",
      "|2020-10-25 22:16:40|    4|\n",
      "|2020-10-25 23:45:00|    5|\n",
      "|2020-10-25 17:00:00|    1|\n",
      "|2020-10-25 18:06:40|    2|\n",
      "|2020-10-25 19:13:20|    2|\n",
      "|2020-10-25 20:20:00|    3|\n",
      "|2020-10-25 21:10:00|    4|\n",
      "|2020-10-25 22:21:40|    4|\n",
      "|2020-10-25 22:55:00|    5|\n",
      "|2020-10-26 00:01:40|    6|\n",
      "|2020-10-26 00:51:40|    7|\n",
      "|2020-10-26 01:08:20|    8|\n",
      "|2020-10-26 01:25:00|    9|\n",
      "+-------------------+-----+\n",
      "\n",
      "+---+\n",
      "|DAU|\n",
      "+---+\n",
      "|  5|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DAU - access\n",
    "spark.sql(\"select a_time as a_time, a_uid from access\").show()\n",
    "dau = spark.sql(\"select count(distinct a_uid) as DAU from access where a_time >= '2020-10-25 00:00:00' and a_time < '2020-10-26 00:00:00'\")\n",
    "dau.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|             p_time|p_uid|\n",
      "+-------------------+-----+\n",
      "|2020-10-25 18:45:50|    1|\n",
      "|2020-10-26 06:45:55|    1|\n",
      "|2020-10-26 00:51:40|    2|\n",
      "|2020-10-25 18:55:55|    3|\n",
      "|2020-10-26 01:08:20|    4|\n",
      "|2020-10-25 22:45:55|    5|\n",
      "|2020-10-25 22:49:15|    5|\n",
      "+-------------------+-----+\n",
      "\n",
      "+---+\n",
      "| PU|\n",
      "+---+\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PU - purchase\n",
    "spark.sql(\"select p_time, p_uid from purchase\").show()\n",
    "pu = spark.sql(\"select count(distinct p_uid) as PU from purchase where p_time >= '2020-10-25 00:00:00' and p_time < '2020-10-26 00:00:00'\")\n",
    "pu.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_dau = dau.collect()[0][\"DAU\"]\n",
    "v_pu = pu.collect()[0][\"PU\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과제 2. 주어진 데이터를 이용하여 2020/10/25 기준의 ARPU, ARPPU 를 구하시오\n",
    "* ARPU : Average Revenue Per User, 유저 당 평균 수익\n",
    "  - 해당 일자의 전체 수익 (Total Purchase Amount) / 해당 일에 접속한 유저 수 (DAU)\n",
    "* ARPPU : Average Revenue Per Purchase User, 구매 유저 당 평균 수익\n",
    "  - 해당 일자의 전체 수익 (Total Purchase Amount) / 해당 일에 접속한 구매 유저 수 (PU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select sum(p_amount) / 5 from purchase where p_time >= '2020-10-25 00:00:00' and p_time < '2020-10-26 00:00:00'\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`p_amount`' given input columns: [purchase.p_name, purchase.p_uid, purchase.p_time, purchase.p_id, purchase.p_amoun]; line 1 pos 11;\\n'Project ['sum('p_amount) AS total_purchase_amount#233]\\n+- Filter ((p_time#106 >= 2020-10-25 00:00:00) && (p_time#106 < 2020-10-26 00:00:00))\\n   +- SubqueryAlias `purchase`\\n      +- Project [from_unixtime(cast(p_time#96 as bigint), yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)) AS p_time#106, p_uid#97, p_id#98, p_name#99, p_amoun#100]\\n         +- Relation[p_time#96,p_uid#97,p_id#98,p_name#99,p_amoun#100] parquet\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o25.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`p_amount`' given input columns: [purchase.p_name, purchase.p_uid, purchase.p_time, purchase.p_id, purchase.p_amoun]; line 1 pos 11;\n'Project ['sum('p_amount) AS total_purchase_amount#233]\n+- Filter ((p_time#106 >= 2020-10-25 00:00:00) && (p_time#106 < 2020-10-26 00:00:00))\n   +- SubqueryAlias `purchase`\n      +- Project [from_unixtime(cast(p_time#96 as bigint), yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)) AS p_time#106, p_uid#97, p_id#98, p_name#99, p_amoun#100]\n         +- Relation[p_time#96,p_uid#97,p_id#98,p_name#99,p_amoun#100] parquet\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$mapChild$2(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$13.apply(TreeNode.scala:356)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:356)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-05a5129a1d83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtotal_purchase_amount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select sum(p_amount) as total_purchase_amount from purchase where p_time >= '2020-10-25 00:00:00' and p_time < '2020-10-26 00:00:00'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtotal_purchase_amount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`p_amount`' given input columns: [purchase.p_name, purchase.p_uid, purchase.p_time, purchase.p_id, purchase.p_amoun]; line 1 pos 11;\\n'Project ['sum('p_amount) AS total_purchase_amount#233]\\n+- Filter ((p_time#106 >= 2020-10-25 00:00:00) && (p_time#106 < 2020-10-26 00:00:00))\\n   +- SubqueryAlias `purchase`\\n      +- Project [from_unixtime(cast(p_time#96 as bigint), yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)) AS p_time#106, p_uid#97, p_id#98, p_name#99, p_amoun#100]\\n         +- Relation[p_time#96,p_uid#97,p_id#98,p_name#99,p_amoun#100] parquet\\n\""
     ]
    }
   ],
   "source": [
    "# ARPU - total purchase amount, dau\n",
    "\n",
    "query=\"select sum(p_amount) / {} from purchase where p_time >= '2020-10-25 00:00:00' and p_time < '2020-10-26 00:00:00'\".format(v_dau)\n",
    "print(query)\n",
    "\n",
    "total_purchase_amount = spark.sql(\"select sum(p_amount) as total_purchase_amount from purchase where p_time >= '2020-10-25 00:00:00' and p_time < '2020-10-26 00:00:00'\")\n",
    "total_purchase_amount.show()\n",
    "\n",
    "spark.sql(\"select sum(p_amount) / 5 from purchase where p_time >= '2020-10-25 00:00:00' and p_time < '2020-10-26 00:00:00'\").show()\n",
    "\n",
    "spark.sql(\"select sum(p_amount) / {} as ARPU from purchase where p_time >= '2020-10-25 00:00:00' and p_time < '2020-10-26 00:00:00'\".format(v_dau)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ARPPU | 3000000.0 |\n"
     ]
    }
   ],
   "source": [
    "# ARPPU - total purchase amount, pu\n",
    "v_amt = total_purchase_amount.collect()[0][\"total_purchase_amount\"]\n",
    "print(\"| ARPPU | {} |\".format(v_amt / v_pu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과제 3. 주어진 데이터를 이용하여 2020/10/26 현재의 \"누적 매출 금액\" 과 \"누적 접속 유저수\"를 구하시오\n",
    "* 누적 매출 금액 : 10/25 (오픈) ~ 현재\n",
    "  - 전체 로그를 읽어서 매출 금액의 합을 구한다\n",
    "  - 유저별 매출 정보를 누적하여 저장해두고 재활용한다\n",
    "* 누적 접속 유저수 : 10/25 (오픈) ~ 현재\n",
    "  - 전체 로그를 읽어서 접속자의 유일한 수를 구한다\n",
    "  - 유저별 접속 정보를 누적하여 저장해두고 재활용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(p_amount)|\n",
      "+-------------+\n",
      "|     16700000|\n",
      "+-------------+\n",
      "\n",
      "+---------------------+\n",
      "|count(DISTINCT a_uid)|\n",
      "+---------------------+\n",
      "|                    9|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 누적 매출 금액\n",
    "spark.sql(\"select sum(p_amount) from purchase \").show()\n",
    "\n",
    "# 누적 접속 유저수\n",
    "spark.sql(\"select count(distinct a_uid) from access\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과제 4. 유저별 정보를 누적시키기 위한 디멘젼 테이블을 설계하고 생성합니다\n",
    "\n",
    "#### User Dimension 테이블 설계\n",
    "| 컬럼 명 | 컬럼 타입 | 컬럼 설명 |\n",
    "| :- | :-: | :- |\n",
    "| d_uid | int | 유저 아이디 |\n",
    "| d_name | string | 고객 이름 |\n",
    "| d_pamount | int | 누적 구매 금액 |\n",
    "| d_pcount | int | 누적 구매 횟수 |\n",
    "| d_acount | int | 누적 접속 횟수 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|  a_time|   string|   null|\n",
      "|   a_uid|      int|   null|\n",
      "|    a_id|   string|   null|\n",
      "+--------+---------+-------+\n",
      "\n",
      "+-------------------+-----+-----+\n",
      "|             a_time|a_uid| a_id|\n",
      "+-------------------+-----+-----+\n",
      "|2020-10-25 17:00:00|    1|login|\n",
      "|2020-10-25 18:06:40|    2|login|\n",
      "|2020-10-25 19:13:20|    2|login|\n",
      "|2020-10-25 20:20:00|    3|login|\n",
      "|2020-10-25 21:10:00|    4|login|\n",
      "|2020-10-25 22:21:40|    4|login|\n",
      "|2020-10-25 22:55:00|    5|login|\n",
      "+-------------------+-----+-----+\n",
      "\n",
      "+-----+------+\n",
      "|a_uid|acount|\n",
      "+-----+------+\n",
      "|    1|     1|\n",
      "|    3|     1|\n",
      "|    5|     1|\n",
      "|    4|     2|\n",
      "|    2|     2|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 오픈 첫 날의 경우 예외적으로 별도의 프로그램을 작성합니다\n",
    "# \n",
    "# 1. 가장 큰 레코드 수를 가진 정보가 접속정보이므로 해당 일자의 이용자 별 접속 횟수를 추출합니다\n",
    "# 단, login 횟수를 접속 횟수로 가정합니다 - logout 만 있는 경우는 login 유실 혹은 전일자의 로그이므로 이러한 경우는 제외합니다\n",
    "spark.sql(\"describe access\").show()\n",
    "spark.sql(\"select * from access where a_id = 'login' and a_time >= '2020-10-25 00:00:00' and a_time < '2020-10-26 00:00:00'\").show()\n",
    "uids = spark.sql(\"select a_uid, count(a_uid) as acount from access where a_id = 'login' and a_time >= '2020-10-25 00:00:00' and a_time < '2020-10-26 00:00:00' group by a_uid\")\n",
    "uids.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|  p_time|   string|   null|\n",
      "|   p_uid|      int|   null|\n",
      "|    p_id|      int|   null|\n",
      "|  p_name|   string|   null|\n",
      "|p_amount|      int|   null|\n",
      "+--------+---------+-------+\n",
      "\n",
      "+-----+-------+------+\n",
      "|p_uid|pamount|pcount|\n",
      "+-----+-------+------+\n",
      "|    1|2000000|     1|\n",
      "|    3|1000000|     1|\n",
      "|    5|6000000|     2|\n",
      "+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. 해당 일자의 이용자 별 총 매출 금액과, 구매 횟수를 추출합니다\n",
    "spark.sql(\"describe purchase\").show()\n",
    "amts = spark.sql(\"select p_uid, sum(p_amount) as pamount, count(p_uid) as pcount from purchase where p_time >= '2020-10-25 00:00:00' and p_time < '2020-10-26 00:00:00' group by p_uid\")\n",
    "amts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a_uid: integer (nullable = true)\n",
      " |-- acount: long (nullable = false)\n",
      "\n",
      "root\n",
      " |-- p_uid: integer (nullable = true)\n",
      " |-- pamount: long (nullable = true)\n",
      " |-- pcount: long (nullable = false)\n",
      "\n",
      "+-----+--------+---------+--------+\n",
      "|d_uid|d_acount|d_pamount|d_pcount|\n",
      "+-----+--------+---------+--------+\n",
      "|    1|       1|  2000000|       1|\n",
      "|    2|       2|     null|    null|\n",
      "|    3|       1|  1000000|       1|\n",
      "|    4|       2|     null|    null|\n",
      "|    5|       1|  6000000|       2|\n",
      "+-----+--------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. 이용자 접속횟수 + 총구매금액 + 구매횟수 (uids + amts)\n",
    "uids.printSchema()\n",
    "amts.printSchema()\n",
    "\n",
    "dim1 = uids.join(amts, uids[\"a_uid\"] == amts[\"p_uid\"], how=\"left\").sort(uids[\"a_uid\"].asc())\n",
    "dim2 = dim1.withColumnRenamed(\"a_uid\", \"d_uid\") \\\n",
    ".withColumnRenamed(\"acount\", \"d_acount\") \\\n",
    ".drop(\"p_uid\") \\\n",
    ".withColumnRenamed(\"pamount\", \"d_pamount\") \\\n",
    ".withColumnRenamed(\"pcount\", \"d_pcount\")\n",
    "dim2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+--------+--------+\n",
      "|u_id|    u_name|u_gender|u_signup|\n",
      "+----+----------+--------+--------+\n",
      "|   1|    정휘센|      남|19580808|\n",
      "|   2|  김싸이언|      남|19590201|\n",
      "|   3|    박트롬|      여|19951030|\n",
      "|   4|    청소기|      남|19770329|\n",
      "|   5|유코드제로|      여|20021029|\n",
      "|   6|  윤디오스|      남|20040101|\n",
      "|   7|  임모바일|      남|20040807|\n",
      "|   8|  조노트북|      여|20161201|\n",
      "|   9|  최컴퓨터|      남|20201124|\n",
      "+----+----------+--------+--------+\n",
      "\n",
      "+-----+----------+--------+--------+---------+--------+\n",
      "|d_uid|    d_name|d_gender|d_acount|d_pamount|d_pcount|\n",
      "+-----+----------+--------+--------+---------+--------+\n",
      "|    1|    정휘센|      남|       1|  2000000|       1|\n",
      "|    2|  김싸이언|      남|       2|        0|       0|\n",
      "|    3|    박트롬|      여|       1|  1000000|       1|\n",
      "|    4|    청소기|      남|       2|        0|       0|\n",
      "|    5|유코드제로|      여|       1|  6000000|       2|\n",
      "+-----+----------+--------+--------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. 이용자 정보를 덧붙입니다\n",
    "user = spark.sql(\"select * from user\")\n",
    "user.show()\n",
    "\n",
    "dim3 = dim2.join(user, dim2[\"d_uid\"] == user[\"u_id\"], \"left\")\n",
    "dim4 = dim3.withColumnRenamed(\"u_name\", \"d_name\") \\\n",
    ".withColumnRenamed(\"u_gender\", \"d_gender\")\n",
    "\n",
    "dim5 = dim4.select(\"d_uid\", \"d_name\", \"d_gender\", \"d_acount\", \"d_pamount\", \"d_pcount\")\n",
    "dimension = dim5.na.fill({\"d_pamount\":0, \"d_pcount\":0})\n",
    "dimension.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 다음날 해당 데이터를 사용하도록 하기 위해 일자별 경로에 저장합니다\n",
    "# - ./users/dt=20201025/\n",
    "target=\"./users/dt=20201025\"\n",
    "dimension.write.mode(\"overwrite\").parquet(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과제 5. 전일자 디멘젼 정보를 이용하여 누적된 접속, 매출 지표를 생성합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+--------+---------+--------+\n",
      "|d_uid|    d_name|d_gender|d_acount|d_pamount|d_pcount|\n",
      "+-----+----------+--------+--------+---------+--------+\n",
      "|    1|    정휘센|      남|       1|  2000000|       1|\n",
      "|    2|  김싸이언|      남|       2|        0|       0|\n",
      "|    3|    박트롬|      여|       1|  1000000|       1|\n",
      "|    4|    청소기|      남|       2|        0|       0|\n",
      "|    5|유코드제로|      여|       1|  6000000|       2|\n",
      "+-----+----------+--------+--------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 이전 일자 기준의 고객의 상태를 유지하여 활용합니다\n",
    "yesterday = spark.read.parquet(target)\n",
    "yesterday.sort(yesterday[\"d_uid\"].asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+--------+---------+--------+\n",
      "|d_uid|    d_name|d_gender|d_acount|d_pamount|d_pcount|\n",
      "+-----+----------+--------+--------+---------+--------+\n",
      "|    5|유코드제로|      여|       1|  6000000|       2|\n",
      "|    2|  김싸이언|      남|       2|        0|       0|\n",
      "|    1|    정휘센|      남|       1|  2000000|       1|\n",
      "|    3|    박트롬|      여|       1|  1000000|       1|\n",
      "|    4|    청소기|      남|       2|        0|       0|\n",
      "+-----+----------+--------+--------+---------+--------+\n",
      "\n",
      "+---+\n",
      "|uid|\n",
      "+---+\n",
      "|  1|\n",
      "|  6|\n",
      "|  3|\n",
      "|  5|\n",
      "|  9|\n",
      "|  4|\n",
      "|  8|\n",
      "|  7|\n",
      "|  2|\n",
      "+---+\n",
      "\n",
      "+----+----------+--------+--------+\n",
      "|u_id|    u_name|u_gender|u_signup|\n",
      "+----+----------+--------+--------+\n",
      "|   1|    정휘센|      남|19580808|\n",
      "|   2|  김싸이언|      남|19590201|\n",
      "|   3|    박트롬|      여|19951030|\n",
      "|   4|    청소기|      남|19770329|\n",
      "|   5|유코드제로|      여|20021029|\n",
      "|   6|  윤디오스|      남|20040101|\n",
      "|   7|  임모바일|      남|20040807|\n",
      "|   8|  조노트북|      여|20161201|\n",
      "|   9|  최컴퓨터|      남|20201124|\n",
      "+----+----------+--------+--------+\n",
      "\n",
      "+---+----------+--------+\n",
      "|uid|    u_name|u_gender|\n",
      "+---+----------+--------+\n",
      "|  1|    정휘센|      남|\n",
      "|  6|  윤디오스|      남|\n",
      "|  3|    박트롬|      여|\n",
      "|  5|유코드제로|      여|\n",
      "|  9|  최컴퓨터|      남|\n",
      "|  4|    청소기|      남|\n",
      "|  8|  조노트북|      여|\n",
      "|  7|  임모바일|      남|\n",
      "|  2|  김싸이언|      남|\n",
      "+---+----------+--------+\n",
      "\n",
      "dim2\n",
      "+---+----------+--------+--------+---------+--------+\n",
      "|uid|    u_name|u_gender|d_acount|d_pamount|d_pcount|\n",
      "+---+----------+--------+--------+---------+--------+\n",
      "|  1|    정휘센|      남|       1|  2000000|       1|\n",
      "|  6|  윤디오스|      남|       0|        0|       0|\n",
      "|  3|    박트롬|      여|       1|  1000000|       1|\n",
      "|  5|유코드제로|      여|       1|  6000000|       2|\n",
      "|  9|  최컴퓨터|      남|       0|        0|       0|\n",
      "|  4|    청소기|      남|       2|        0|       0|\n",
      "|  8|  조노트북|      여|       0|        0|       0|\n",
      "|  7|  임모바일|      남|       0|        0|       0|\n",
      "|  2|  김싸이언|      남|       2|        0|       0|\n",
      "+---+----------+--------+--------+---------+--------+\n",
      "\n",
      "+-----+------+\n",
      "|a_uid|acount|\n",
      "+-----+------+\n",
      "|    6|     1|\n",
      "|    9|     1|\n",
      "|    8|     1|\n",
      "|    7|     1|\n",
      "+-----+------+\n",
      "\n",
      "dim3\n",
      "+---+----------+--------+--------+---------+--------+\n",
      "|uid|    u_name|u_gender|d_acount|d_pamount|d_pcount|\n",
      "+---+----------+--------+--------+---------+--------+\n",
      "|  1|    정휘센|      남|       1|  2000000|       1|\n",
      "|  6|  윤디오스|      남|       1|        0|       0|\n",
      "|  3|    박트롬|      여|       1|  1000000|       1|\n",
      "|  5|유코드제로|      여|       1|  6000000|       2|\n",
      "|  9|  최컴퓨터|      남|       1|        0|       0|\n",
      "|  4|    청소기|      남|       2|        0|       0|\n",
      "|  8|  조노트북|      여|       1|        0|       0|\n",
      "|  7|  임모바일|      남|       1|        0|       0|\n",
      "|  2|  김싸이언|      남|       2|        0|       0|\n",
      "+---+----------+--------+--------+---------+--------+\n",
      "\n",
      "+---+----------+--------+--------+---------+--------+\n",
      "|uid|    u_name|u_gender|d_acount|d_pamount|d_pcount|\n",
      "+---+----------+--------+--------+---------+--------+\n",
      "|  1|    정휘센|      남|       1|  2000000|       1|\n",
      "|  6|  윤디오스|      남|       1|        0|       0|\n",
      "|  3|    박트롬|      여|       1|  1000000|       1|\n",
      "|  5|유코드제로|      여|       1|  6000000|       2|\n",
      "|  9|  최컴퓨터|      남|       1|        0|       0|\n",
      "|  4|    청소기|      남|       2|        0|       0|\n",
      "|  8|  조노트북|      여|       1|        0|       0|\n",
      "|  7|  임모바일|      남|       1|        0|       0|\n",
      "|  2|  김싸이언|      남|       2|        0|       0|\n",
      "+---+----------+--------+--------+---------+--------+\n",
      "\n",
      "+-----+-------+------+\n",
      "|p_uid|pamount|pcount|\n",
      "+-----+-------+------+\n",
      "|    1|1800000|     1|\n",
      "|    4|4500000|     1|\n",
      "|    2|1400000|     1|\n",
      "+-----+-------+------+\n",
      "\n",
      "dim4\n",
      "+-----+----------+--------+--------+---------+--------+\n",
      "|d_uid|    d_name|d_gender|d_acount|d_pamount|d_pcount|\n",
      "+-----+----------+--------+--------+---------+--------+\n",
      "|    1|    정휘센|      남|       1|  3800000|       2|\n",
      "|    2|  김싸이언|      남|       2|  1400000|       3|\n",
      "|    3|    박트롬|      여|       1|  1000000|       1|\n",
      "|    4|    청소기|      남|       2|  4500000|       3|\n",
      "|    5|유코드제로|      여|       1|  6000000|       1|\n",
      "|    6|  윤디오스|      남|       1|        0|       1|\n",
      "|    7|  임모바일|      남|       1|        0|       1|\n",
      "|    8|  조노트북|      여|       1|        0|       1|\n",
      "|    9|  최컴퓨터|      남|       1|        0|       1|\n",
      "+-----+----------+--------+--------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. 다음 날 동일한 지표를 생성하되 이전 일자의 정보에 누적한 지표를 생성합니다\n",
    "# 기존 테이블의 고객과 오늘 신규 고객을 모두 포함한 전체 데이터집합을 생성합니다\n",
    "yesterday.show()\n",
    "\n",
    "# 새로운 모수를 추가해야 하므로 전체 모수에 해당하는 uid 만을 추출합니다\n",
    "uid = yesterday.select(\"d_uid\").join(accs.select(\"a_uid\"), yesterday.d_uid == accs.a_uid, \"full_outer\") \\\n",
    ".withColumn(\"uid\", when(yesterday.d_uid.isNull(), accs.a_uid).otherwise(yesterday.d_uid)) \\\n",
    ".select(\"uid\")\n",
    "uid.show()\n",
    "\n",
    "# uid 기준으로 이름, 성별을 조인합니다\n",
    "user.show()\n",
    "dim1 = uid.join(user, uid.uid == user.u_id).select(uid.uid, user.u_name, user.u_gender)\n",
    "dim1.show()\n",
    "\n",
    "# 어제 디멘젼을 기준으로 누적접속, 누적구매금액, 누적구매횟수 등을 조인합니다\n",
    "print(\"dim2\")\n",
    "dim2 = dim1.join(yesterday, dim1.uid == yesterday.d_uid, \"left\") \\\n",
    ".select(dim1.uid, dim1.u_name, dim1.u_gender, yesterday.d_acount, yesterday.d_pamount, yesterday.d_pcount) \\\n",
    ".na.fill({\"d_acount\":0, \"d_pamount\":0, \"d_pcount\":0})\n",
    "\n",
    "dim2.show()\n",
    "\n",
    "# 6. 오늘 생성된 접속수치, 매출 및 매출 횟수를 더합니다 \n",
    "accs = spark.sql(\"select a_uid, count(a_uid) as acount from access where a_id = 'login' and a_time >= '2020-10-26 00:00:00' and a_time < '2020-10-27 00:00:00' group by a_uid\")\n",
    "accs.show()\n",
    "\n",
    "print(\"dim3\")\n",
    "dim3 = dim2.join(accs, dim2.uid == accs.a_uid, \"left\") \\\n",
    ".withColumn(\"total_amount\", dim2.d_acount + when(accs.acount.isNull(), 0).otherwise(accs.acount)) \\\n",
    ".select(\"uid\", \"u_name\", \"u_gender\", \"total_amount\", \"d_pamount\", \"d_pcount\") \\\n",
    ".withColumnRenamed(\"total_amount\", \"d_acount\")\n",
    "\n",
    "dim3.show()\n",
    "\n",
    "# 오늘 발생한 매출을 더합니다\n",
    "dim3.show()\n",
    "\n",
    "amts = spark.sql(\"select p_uid, sum(p_amount) as pamount, count(p_uid) as pcount from purchase where p_time >= '2020-10-26 00:00:00' and p_time < '2020-10-27 00:00:00' group by p_uid\")\n",
    "amts.show()\n",
    "\n",
    "print(\"dim4\")\n",
    "dim4 = dim3.join(amts, dim3.uid == amts.p_uid, \"left\") \\\n",
    ".withColumn(\"total_pamount\", dim3.d_pamount + when(amts.pamount.isNull(), 0).otherwise(amts.pamount)) \\\n",
    ".withColumn(\"total_pcount\", dim3.d_acount + when(amts.pcount.isNull(), 0).otherwise(amts.pcount)) \\\n",
    ".drop(\"d_pamount\", \"d_pcount\") \\\n",
    ".withColumnRenamed(\"uid\", \"d_uid\") \\\n",
    ".withColumnRenamed(\"u_name\", \"d_name\") \\\n",
    ".withColumnRenamed(\"u_gender\", \"d_gender\") \\\n",
    ".withColumnRenamed(\"total_pamount\", \"d_pamount\") \\\n",
    ".withColumnRenamed(\"total_pcount\", \"d_pcount\") \\\n",
    ".select(\"d_uid\", \"d_name\", \"d_gender\", \"d_acount\", \"d_pamount\", \"d_pcount\")\n",
    "\n",
    "dimension = dim4.sort(dim4.d_uid.asc()).coalesce(1)\n",
    "dimension.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 생성된 디멘젼을 20201026 경로에 저장합니다\n",
    "target=\"./users/dt=20201026\"\n",
    "dimension.write.mode(\"overwrite\").parquet(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과제 6. inner, left_outer, right_outer, full_outer 조인 실습 예제를 작성하시오\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+-------+\n",
      "|a_id|a_value|b_id|b_value|\n",
      "+----+-------+----+-------+\n",
      "|   C|      3|   C|     10|\n",
      "|   D|      4|   D|     20|\n",
      "+----+-------+----+-------+\n",
      "\n",
      "+----+-------+----+-------+\n",
      "|a_id|a_value|b_id|b_value|\n",
      "+----+-------+----+-------+\n",
      "|   A|      1|null|   null|\n",
      "|   B|      2|null|   null|\n",
      "|   C|      3|   C|     10|\n",
      "|   D|      4|   D|     20|\n",
      "+----+-------+----+-------+\n",
      "\n",
      "+----+-------+----+-------+\n",
      "|a_id|a_value|b_id|b_value|\n",
      "+----+-------+----+-------+\n",
      "|   C|      3|   C|     10|\n",
      "|   D|      4|   D|     20|\n",
      "|null|   null|   E|     30|\n",
      "|null|   null|   F|     40|\n",
      "+----+-------+----+-------+\n",
      "\n",
      "+----+-------+----+-------+\n",
      "|a_id|a_value|b_id|b_value|\n",
      "+----+-------+----+-------+\n",
      "|   A|      1|null|   null|\n",
      "|   B|      2|null|   null|\n",
      "|   C|      3|   C|     10|\n",
      "|   D|      4|   D|     20|\n",
      "|null|   null|   E|     30|\n",
      "|null|   null|   F|     40|\n",
      "+----+-------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valuesA = [('A',1),('B',2),('C',3),('D',4)]\n",
    "A = spark.createDataFrame(valuesA,['a_id','a_value'])\n",
    " \n",
    "valuesB = [('C',10),('D',20),('E',30),('F',40)]\n",
    "B = spark.createDataFrame(valuesB,['b_id','b_value'])\n",
    "\n",
    "A.join(B, A.a_id == B.b_id, \"inner\").sort(A.a_id.asc()).show() # C, D\n",
    "# A.join(B, A.a_id == B.b_id, \"left\").sort(A.a_id.asc()).show() # A, B, C, D\n",
    "# A.join(B, A.a_id == B.b_id, \"right\").sort(B.b_id.asc()).show() # C, D, E, F\n",
    "A.join(B, A.a_id == B.b_id, \"left_outer\").sort(A.a_id.asc()).show() # A, B, C, D\n",
    "A.join(B, A.a_id == B.b_id, \"right_outer\").sort(B.b_id.asc()).show() # C, D, E, F\n",
    "A.join(B, A.a_id == B.b_id, \"full_outer\").sort(A.a_id.asc_nulls_last(), B.b_id.asc_nulls_last()).show() # A, B, C, D, E, F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  A|\n",
      "|  B|\n",
      "|  C|\n",
      "|  D|\n",
      "|  E|\n",
      "|  F|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# full outer 조인 시에 결과 생성\n",
    "A.join(B, A.a_id == B.b_id, \"full_outer\").withColumn(\"id\", expr(\"case when a_id is null then b_id else a_id end\")).select(\"id\").sort(\"id\").show()\n",
    "# F.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+-------+\n",
      "|a_id|a_value|b_id|b_value|\n",
      "+----+-------+----+-------+\n",
      "|null|   null|   F|     40|\n",
      "|null|   null|   E|     30|\n",
      "|   B|      2|null|   null|\n",
      "|   D|      4|   D|     20|\n",
      "|   C|      3|   C|     10|\n",
      "|   A|      1|null|   null|\n",
      "+----+-------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A.join(B, A.a_id == B.b_id, \"full_outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  A|\n",
      "|  B|\n",
      "|  C|\n",
      "|  D|\n",
      "|  E|\n",
      "|  F|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A.join(B, A.a_id == B.b_id, \"full_outer\").withColumn(\"id\", when(A.a_id.isNull(), B.b_id).otherwise(A.a_id)).select(\"id\").sort(\"id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과제 7. 전일자 디멘젼 정보와 오늘자 로그를 이용하여 팩트 데이터를 생성합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과제 8. 팩트 데이터를 이용하여 2020/10/25 기준 성별 매출금액 지표를 추출합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
